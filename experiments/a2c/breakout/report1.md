# A2C Breakout
## Tests
Reward: Natural reward per life. Not clipped reward. Not reward per episode (5 lives)

X axis: Number of update

### test2
|name|value|
|----|-----|
|entropyCoef|0.01|
|valueCoef|0.5|
|maxGradNormClip|0.1|
|reward|[-1000, 1000]|
|maxStep|16|
|envNum|50|
|model|AirACCnnNet|

The extended reward scope led to divergence. The value loss did not explore but entropy indicated that unexpected reward destroyed the model. The model recovered after about 2,000 episodes.

![reward](./docs/boa_test2/reward.jpg)

![value](./docs/boa_test2/value.jpg)

![entropy](./docs/boa_test2/entropy.jpg)

### test4
|name|value|
|----|-----|
|entropyCoef|0.01|
|valueCoef|0.5|
|maxGradNormClip|0.1|
|reward|[-1, 1]|
|normReward|false|
|maxStep|8|
|envNum|50|
|model|AirACCnnNet|

Similar to test3. *normReward* did not matter. *maxStep* did not help.

### test5
|name|value|
|----|-----|
|entropyCoef|0.01|
|valueCoef|0.5|
|maxGradNormClip|0.1|
|reward|[-1000, 1000]|
|normReward|true|
|maxStep|20|
|envNum|50|
|model|AirACCnnNet|

Worked, but not stable. Still rather unstable entropy in beginning.

![reward](./docs/boa_test5/reward.jpg)

![entropy](./docs/boa_test5/entropy.jpg)

### test8
|name|value|
|----|-----|
|entropyCoef|0.01|
|valueCoef|0.5|
|maxGradNormClip|0.1|
|reward|[-1, 1]|
|normReward|false|
|maxStep|40|
|envNum|50|
|model|AirACCnnNet|

Seemed variance introduced by *maxStep = 40* is hard to converge

![reward](./docs/boa_test8/reward.jpg)

### test9/test10/test11
|name|value|
|----|-----|
|entropyCoef|0.01|
|valueCoef|0.5|
|maxGradNormClip|0.5|
|reward|[-100, 100]|
|normReward|false|
|maxStep|10|
|envNum|50|
|model|AirACCnnNet|

Set reward scope as [-100, 100] hoping the agent learns most efficient actions.

After continuous training:

![reward](./docs/boa_test9/reward_9_10_11.jpg)

### test13/test14
Continue training model generated by test11. Each training got improvement, but slow and unstable.

![reward](./docs/boa_test13/reward_13_14.jpg)

### test15
Continue training model generated by test14. Still improved and unstable. 
Maybe the dominating value loss pushed the agent being hard to explore.

![reward](./docs/boa_test15/reward.jpg)

![value_loss](./docs/boa_test15/value_loss.jpg)

![entropy](./docs/boa_test15/entropy.jpg)

### test17
Continue training model generated by test15. Seemed no better than result of test15

![reward](./docs/boa_test17/reward.jpg)

### test16
|name|value|
|----|-----|
|entropyCoef|0.01|
|valueCoef|0.1|
|maxGradNormClip|0.5|
|reward|[-100, 100]|
|normReward|false|
|maxStep|20|
|envNum|50|
|model|AirACCnnNet|

To reduce the significance of value loss, decrease the *valueCoef*. Also try increasing *maxStep*.

The reward indicated that learning was slow and unstable. The value loss has been restrained, but the entropy indicated that the agent was not learning.

![reward](./docs/boa_test16/reward.jpg)

![value_loss](./docs/boa_test16/value_loss.jpg)

![entropy](./docs/boa_test16/entropy.jpg)


### test18
|name|value|
|----|-----|
|entropyCoef|0.01|
|valueCoef|0.5|
|maxGradNormClip|0.5|
|reward|[-100, 100]|
|normReward|false|
|maxStep|10|
|envNum|80|
|model|AirACCnnNet|

Increased batchSize(envNum) to increase stability based on model generated by test17. It was better than the result of case envNum = 50.

![reward](./docs/boa_test18/reward.jpg)

The test result is not bad:

Reward for each life:

![reward_life](./docs/boa_test18/test_reward_life_65.jpg)

Reward for each episode (5 lives):

![reward_episode](./docs/boa_test18/test_reward_sum_65.jpg)

### test0
Turn to [-1, 1] reward.

|name|value|
|----|-----|
|entropyCoef|0.01|
|valueCoef|0.5|
|maxGradNormClip|0.1|
|reward|[-1, 1]|
|maxStep|16|
|envNum|50|
|model|AirACCnnNet|

Reward is promising.

![reward](./docs/boa_test0/reward.jpg)

### test1
|name|value|
|----|-----|
|entropyCoef|0.01|
|valueCoef|0.25|
|maxGradNormClip|0.1|
|reward|[-1, 1]|
|maxStep|16|
|envNum|50|
|model|AirACCnnNet|

Reward trapped around 25. Entropy seemed reasonable.
Seemed (value = 0.25) made value estimation fails to catch up with change of input.
Or maybe it requires more training episodes.

![reward](./docs/boa_test1/reward.jpg)

![entropy](./docs/boa_test1/entropy.jpg)

### test3
|name|value|
|----|-----|
|entropyCoef|0.01|
|valueCoef|0.5|
|maxGradNormClip|0.1|
|reward|[-1, 1]|
|normReward|true|
|maxStep|16|
|envNum|50|
|model|AirACCnnNet|

Continue training on test0 model, no improvement. While there were about only 1,000 episodes, can not prove that it is a bad model.

![reward](./docs/boa_test3/reward.jpg)

### test201/test202
|name|value|
|----|-----|
|entropyCoef|0.01|
|valueCoef|0.5|
|maxGradNormClip|0.5|
|reward|[-1, 1]|
|normReward|false|
|maxStep|16|
|envNum|50|
|model|AirACCnnNet|


Continue training model generated by test3. Reward was measured by number of steps per life.

![reward](./docs/boa_test201/reward.jpg)

![reward](./docs/boa_test202/reward.jpg)

### test203
|name|value|
|----|-----|
|entropyCoef|0.01|
|valueCoef|0.5|
|maxGradNormClip|0.5|
|reward|[-1, 1]|
|normReward|false|
|maxStep|20|
|envNum|50|
|model|AirACHONet|

Increase *maxStep* and model capacity. 

The statistic file had been destroyed.

### test204
|name|value|
|----|-----|
|entropyCoef|0.01|
|valueCoef|0.5|
|maxGradNormClip|0.5|
|reward|[-1, 1]|
|normReward|false|
|maxStep|20|
|envNum|50|
|model|AirACHONet|

Continue training model generated by test203

While not stable, the reward the agent learned was good.
The value_loss was larger than expected, but entropy and action_loss matched the expectation.

![reward](./docs/boa_test204/reward.jpg)

![value_loss](./docs/boa_test204/value_loss.jpg)

![entropy](./docs/boa_test204/entropy.jpg)

![action_loss](./docs/boa_test204/action_loss.jpg)

And test result of reward per episode(5 lives)

![reward_episode](./docs/boa_test204/reward_episode.jpg)

### test205
Continue training model generated by test204. Seemed no obvious improvement.

![reward](./docs/boa_test205/reward.jpg)

Test result indicated that model of the best average reward per life is not same as that of the best average reward per episode(5 lives):

* The best average reward per life = 120
* The best average reward per episode = 420

Average reward per life of 120 case:

![reward_life_120](./docs/boa_test205/ave_reward_120_test.jpg)

Average reward per episode of 120 case:

![reward_episode_120](./docs/boa_test205/ave_reward_120_sum.jpg)

Average reward per life of 420 case:

![reward_life_420](./docs/boa_test205/ave_reward_420_test.jpg)

Average reward per episode of 420 case:

![reward_episode_420](./docs/boa_test205/ave_reward_420_sum.jpg)
## Conclusion
* For A2C, the actor and critical parts requires more capacity than only 1 output layer.
* Too many steps per update makes the model hard to converge. Maybe larger batch size could solve it.
* Seemed this game is to learn how to keep living instead of smart strategy (e.g. get more reward per step). Constraint reward in [-1, 1] makes the model easier to learn. 
* For reason mentioned above, the agent is easy to hang at the end of episode. That is, agent knows how to keep the ball in the air but does not know the target of hitting the brick.
* Larger batch size helps.



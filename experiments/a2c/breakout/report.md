# A2C Experiments
## CartPole
Batch = 32

For MaxStep = 8 (TD(8)?), agent learned stable optimal policy after about 7,000 episodes. 
For MaxStep = 1, agent needs more steps.

![average rewards](images/cartpole_ave_reward_vs.jpg)

For MaxStep = 8 case, the loss is huge and value loss dominated total loss:

![value_altitule](./images/cartpole_a2cn_value_loss_altitude.jpg)

![value_ratio](./images/cartpole_a2cn_value_loss_ratio.jpg)

Although value loss is large, action loss is relatively small.
This indicated that log_softmax(action) was small, and the agent is rather certain on action selection.

![value_loss](./images/cartpole_a2cn_value_loss.jpg)

![action_loss](./images/cartpole_a2cn_action_loss.jpg)

The entropy is stable and 1.3 > entropy > 0.3. 
* entropy coef is small (0.001)
* value + action loss is large
* number of actions = 2

Therefore, the entropy had been pushed into certainty to some degree, 
while not been pushed too much.

![entropy](./images/cartpole_a2cn_entropy.jpg)

## Breakout
### Lesson learned
* Normalize input: observation from Gym is in range of [0, 255], to scale them in [0, 1].
Running mean had also been tried, leading worse performance. Maybe it should be tested by more steps as RSM is unstable at the beginning. Or there was something wrong in my implementation.
* Normalize reward: large reward gap may cause huge advantage and network exploitation
* To works with environment "BreakoutNoFrameskip-v4", other versions are harder to train. *???(link)*

### test4
|name|value|
|---|---|
|batchsize|32|
|episode|102400|
|entropy coef|0.001|
|max step|8|

![ave_reward](./images/a2cnbatch_test4_ave_reward.jpg)

The reward trapped around 4

### test5
|name|value|
|---|---|
|batchsize|32|
|entropy coef|0.001|
|max step|16|
Not completed

### test6
Network with normbatch.

Bad performance

### test7
Network with 4 Conv layers and RSM. Huge value loss at very beginning

|seq|loss|value loss|action loss|entropy|
|---|----|----------|-----------|-------|
|1|4.20273|3.13428|1.06973|1.28127|
|2|809071|809071|-0|-0|
|3|211.014|213.822|-2.80799|0.305423|
|4|13783.8|13783.8|-0|2.69e-07|
|5|1.05072|1.05072|8.69663e-07|1.61256e-05|

### test10
Reward clip [-10, 10] hurt performance, range [-1, 1] better

![reward_scale](./images/a2cnbatch_reward_ave_reward.jpg)

### test11
Continue training test10 model, nothing more learned:

![test11_ave_reward](./images/a2cnbatch_test11_ave_reward.jpg)

### test12
Seemed entropy is too small to explore in test11. Reduce eocf into 0.01 and still continue training model trained by test10:

![test12_ave_reward](./images/a2cnbatch_test12_ave_reward.jpg)

Continue training model generated by test12, it was trapped or exploring:

![test13_ave_reward](./images/a2cnbatch_test13_ave_reward.jpg)

### test14
Try to reduce coef of value loss to push action distinguish, with model generated by test13 

The performance improved but still trapped:

![value coef](./images/a2cnbatch_value_ave_reward.jpg)

### test15
Try to extend reward range to distinguish good action and better action, no better result:

![reward scope](./images/a2cnbatch_reward_010_ave_reward.jpg)

Continue training model generated by test15 leads to some improvement, but still no better than [-1, 1] cases.

![test18](./images/a2cnbatch_test18_ave_reward.jpg)

### test17
Try smaller steps to make keeping living as the ultimate target. 

Seemed that step = 8 is better:

![step_ave_reward](./images/a2cnbatch_step_ave_reward.jpg)

### Conclusion:
Try more training on:

|name|value|
|----|-----|
|batch size|32|
|max step|8|
|entropy|0.01|
|value|0.25|
|reward|[-1, 1]|
|input|/255|

# DQN
## Pong
### test1
|name|value|
|----|-----|
|lr|0.0001|
|rbCap|10240|
|startStep|100|
|exploreBegin|1|
|exploreEnd|0.01|
|explorePart|0.8|
|gamma|0.99|
|batchSize|32|
|inputScale|1|
|rewardScale|1|
|rewardMin|-1|
|rewardMax|1|

The result is promising:

![test1_reward](./images/dqn_test1_reward.jpg)

### test2
|name|value|
|----|-----|
|lr|0.0001|
|rbCap|10240|
|startStep|100|
|exploreBegin|0.5|
|exploreEnd|0.01|
|explorePart|0.8|
|gamma|0.99|
|batchSize|32|
|inputScale|1|
|rewardScale|1|
|rewardMin|-1|
|rewardMax|1|

Continue to train model generated by test1:

![test2_reward](./images/dqn_test2_reward.jpg)

The agent has learned how to win.

### test3
|name|value|
|----|-----|
|lr|0.0001|
|rbCap|10240|
|startStep|100|
|exploreBegin|0.2|
|exploreEnd|0.01|
|explorePart|1|
|gamma|0.99|
|batchSize|32|
|inputScale|1|
|rewardScale|1|
|rewardMin|-1|
|rewardMax|1|

Continue training model generated by test2:

![test3_reward](./images/dqn_test3_reward.jpg)

With greedyEpsilon > 0.8, the agent can never lose. But the max reward is less than that of test2.
It may be because of lack of exploration at the beginning, or randomness at the end of the training.

### test4
|name|value|
|----|-----|
|lr|0.0001|
|rbCap|10240|
|startStep|100|
|exploreBegin|0.5|
|exploreEnd|0.01|
|explorePart|0.8|
|gamma|0.99|
|batchSize|32|
|inputScale|1|
|rewardScale|1|
|rewardMin|-1|
|rewardMax|1|

Continue training model generated by test2:

![test4_reward](./images/dqn_test4_reward.jpg)

Compared to test3, with same update step (1e6), test4 got better performance with more exploration at beginning and more exploitation at end of training.
The curves shown in both cases kept increasing even at the last episode. It may reach max reward (21) with more update steps as theory suggested.

### test result
![test_result](./images/dqn_test_model_reward.jpg)

The test result shows that test2 model is good enough, more episode training (test4) does not help;
exploitation on a good enough model (test2) does not help (test3)

## Breakout
Not worked out yet.
# PPO Resources

## Reference

* [PPO Interpretation](https://stackoverflow.com/questions/46422845/what-is-the-way-to-understand-proximal-policy-optimization-algorithm-in-rl)

* [CartPole PPO](https://github.com/4kasha/CartPole_PPO)

* [A pong params ref](https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ppo/pong-ppo.yaml)

* [GAE](https://towardsdatascience.com/generalized-advantage-estimate-maths-and-code-b5d5bd3ce737)

* [A Continuous Pytorch PPO](https://github.com/nikhilbarhate99/PPO-PyTorch/blob/master/PPO.py)

* [RLlib ppo](https://docs.ray.io/en/master/rllib-algorithms.html#proximal-policy-optimization-ppo)

* [Ray Project](https://github.com/ray-project/rl-experiments)

* [Spinup](https://spinningup.openai.com/en/latest/algorithms/ppo.html)

* [Hyperparameters range](https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe)

* [KL](http://joschu.net/blog/kl-approx.html)

* [Value clip](https://github.com/openai/baselines/issues/91)

* [PPO some feedback](https://github.com/openai/baselines/issues/445)

* [PPO value loss](https://github.com/openai/baselines/issues/91)

* [PPO pong 1](http://www.sagargv.com/blog/pong-ppo/)

* [PPO stackoverflow](https://stackexchange.com/search?q=ppo)

* [A Zhihu reproduce](https://zhuanlan.zhihu.com/p/50322028)

## TODO

* Distributed PPO

* [Deceptive Gradient](https://arxiv.org/pdf/2006.08505.pdf)

* Quality Deversity

* [Exploration Topic](https://stackoverflow.com/questions/63047930/reinforcement-learning-driving-around-objects-with-ppo)

* Difference betweeen initializer: orthogonal, xavier, kaiming are all suggested

* ACKTR
